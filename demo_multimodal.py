"""
SAFESPACE AI AGENT - Multimodal Demo Script

This script demonstrates all the multimodal capabilities of the SAFESPACE AI AGENT:
1. Text-based mental health conversations
2. Image analysis for art therapy and emotional insights
3. Voice input processing and speech-to-text
4. Voice output generation and text-to-speech
5. Multimodal interactions combining text, image, and voice

Run this script to test all features and see the system in action.
"""

import asyncio
import os
import tempfile
from pathlib import Path
import requests
import json

# Import our modules
from core.agent import graph, SYSTEM_PROMPT, parse_response
from core.audio_processor import audio_processor
from core.tools import (
    analyze_uploaded_image,
    process_voice_message,
    generate_voice_response,
    emotional_image_analysis,
    get_general_health_answer
)
from langchain_core.messages import HumanMessage, SystemMessage
from backend.config import GROQ_API_KEY, ELEVENLABS_API_KEY


class MultimodalDemo:
    """
    Demonstration class for SAFESPACE AI AGENT multimodal capabilities.
    """
    
    def __init__(self):
        self.chat_history = [SystemMessage(content=SYSTEM_PROMPT)]
        self.demo_results = {}\n    \n    def print_section(self, title: str):\n        \"\"\"Print a formatted section header.\"\"\"\n        print(\"\\n\" + \"=\"*60)\n        print(f\" {title} \")\n        print(\"=\"*60)\n    \n    def print_result(self, subtitle: str, content: str):\n        \"\"\"Print a formatted result.\"\"\"\n        print(f\"\\nüìã {subtitle}:\")\n        print(\"-\" * 40)\n        print(content)\n        print(\"-\" * 40)\n    \n    async def demo_text_conversation(self):\n        \"\"\"Demonstrate text-based mental health conversation.\"\"\"\n        self.print_section(\"TEXT CONVERSATION DEMO\")\n        \n        test_queries = [\n            \"I'm feeling really anxious about work lately\",\n            \"Can you help me with some breathing exercises?\",\n            \"I've been having trouble sleeping and feeling overwhelmed\"\n        ]\n        \n        for i, query in enumerate(test_queries, 1):\n            print(f\"\\nüó£Ô∏è  Test Query {i}: {query}\")\n            \n            self.chat_history.append(HumanMessage(content=query))\n            inputs = {\"messages\": self.chat_history}\n            stream = graph.astream(inputs, stream_mode=\"updates\")\n            \n            tool_called, response = await parse_response(stream)\n            self.chat_history.append(HumanMessage(content=response))\n            \n            self.print_result(f\"AI Response (Tool: {tool_called})\", response)\n            \n            self.demo_results[f\"text_query_{i}\"] = {\n                \"query\": query,\n                \"response\": response,\n                \"tool_used\": tool_called\n            }\n    \n    def demo_image_analysis(self):\n        \"\"\"Demonstrate image analysis capabilities.\"\"\"\n        self.print_section(\"IMAGE ANALYSIS DEMO\")\n        \n        # Note: In a real scenario, you would have actual image files\n        # For demo purposes, we'll simulate the process\n        \n        print(\"\\nüì∑ Image Analysis Simulation:\")\n        print(\"In a real scenario, you would:\")\n        print(\"1. Upload an image file (drawing, photo, artwork)\")\n        print(\"2. The system would process it through GROQ's vision model\")\n        print(\"3. Get therapeutic insights and emotional analysis\")\n        \n        # Simulate image analysis\n        sample_queries = [\n            \"Analyze this drawing I made when feeling sad\",\n            \"What emotions do you see in this artwork?\",\n            \"Can you provide art therapy insights for this image?\"\n        ]\n        \n        for i, query in enumerate(sample_queries, 1):\n            print(f\"\\nüñºÔ∏è  Sample Image Query {i}: {query}\")\n            print(\"üìù Simulated Analysis Response:\")\n            print(\"I can see emotional expression through color choices and composition...\")\n            print(\"This appears to reflect feelings of introspection and contemplation...\")\n            \n            self.demo_results[f\"image_query_{i}\"] = {\n                \"query\": query,\n                \"simulation\": True,\n                \"note\": \"Requires actual image file and GROQ API key\"\n            }\n    \n    def demo_voice_processing(self):\n        \"\"\"Demonstrate voice input and output capabilities.\"\"\"\n        self.print_section(\"VOICE PROCESSING DEMO\")\n        \n        print(\"\\nüé§ Voice Input Simulation:\")\n        print(\"In a real scenario, you would:\")\n        print(\"1. Record audio using the microphone\")\n        print(\"2. Transcribe speech to text using GROQ Whisper\")\n        print(\"3. Process the text through the AI agent\")\n        print(\"4. Generate voice response using gTTS or ElevenLabs\")\n        \n        # Simulate voice processing workflow\n        sample_voice_text = \"I'm feeling really stressed about my upcoming presentation\"\n        \n        print(f\"\\nüó£Ô∏è  Simulated Voice Input: '{sample_voice_text}'\")\n        \n        # Test text-to-speech generation\n        print(\"\\nüîä Testing Text-to-Speech Generation...\")\n        \n        test_response = \"I understand you're feeling stressed about your presentation. That's completely normal. Let's work through some strategies to help you feel more confident.\"\n        \n        try:\n            # Test gTTS\n            audio_file_gtts = audio_processor.text_to_speech_gtts(test_response)\n            if audio_file_gtts:\n                print(f\"‚úÖ gTTS audio generated: {audio_file_gtts}\")\n                self.demo_results[\"gtts_generation\"] = {\n                    \"status\": \"success\",\n                    \"file\": audio_file_gtts\n                }\n            else:\n                print(\"‚ùå gTTS generation failed\")\n                self.demo_results[\"gtts_generation\"] = {\"status\": \"failed\"}\n            \n            # Test ElevenLabs (if API key is available)\n            if ELEVENLABS_API_KEY:\n                print(\"\\nüéµ Testing ElevenLabs premium voice...\")\n                audio_file_eleven = audio_processor.text_to_speech_elevenlabs(test_response)\n                if audio_file_eleven:\n                    print(f\"‚úÖ ElevenLabs audio generated: {audio_file_eleven}\")\n                    self.demo_results[\"elevenlabs_generation\"] = {\n                        \"status\": \"success\",\n                        \"file\": audio_file_eleven\n                    }\n                else:\n                    print(\"‚ùå ElevenLabs generation failed\")\n                    self.demo_results[\"elevenlabs_generation\"] = {\"status\": \"failed\"}\n            else:\n                print(\"‚ö†Ô∏è  ElevenLabs API key not configured - skipping premium voice test\")\n                self.demo_results[\"elevenlabs_generation\"] = {\"status\": \"skipped\", \"reason\": \"No API key\"}\n        \n        except Exception as e:\n            print(f\"‚ùå Voice generation error: {e}\")\n            self.demo_results[\"voice_generation_error\"] = str(e)\n    \n    def demo_audio_info(self):\n        \"\"\"Demonstrate audio processing capabilities.\"\"\"\n        self.print_section(\"AUDIO PROCESSING CAPABILITIES\")\n        \n        print(\"\\nüîß Audio Processor Capabilities:\")\n        print(\"‚Ä¢ Recording from microphone with configurable duration\")\n        print(\"‚Ä¢ Format conversion (WAV, MP3, FLAC, etc.)\")\n        print(\"‚Ä¢ Speech-to-text using GROQ Whisper or Google Speech Recognition\")\n        print(\"‚Ä¢ Text-to-speech using gTTS or ElevenLabs\")\n        print(\"‚Ä¢ Audio playback and file management\")\n        \n        # Test audio processor initialization\n        try:\n            info = {\n                \"sample_rate\": audio_processor.sample_rate,\n                \"chunk_size\": audio_processor.chunk_size,\n                \"channels\": audio_processor.channels,\n                \"groq_available\": audio_processor.groq_client is not None\n            }\n            self.print_result(\"Audio Processor Configuration\", json.dumps(info, indent=2))\n            self.demo_results[\"audio_processor_config\"] = info\n        except Exception as e:\n            print(f\"‚ùå Audio processor error: {e}\")\n            self.demo_results[\"audio_processor_error\"] = str(e)\n    \n    def demo_configuration_status(self):\n        \"\"\"Show configuration status for all services.\"\"\"\n        self.print_section(\"CONFIGURATION STATUS\")\n        \n        config_status = {\n            \"GROQ API\": \"‚úÖ Configured\" if GROQ_API_KEY else \"‚ùå Not configured\",\n            \"ElevenLabs API\": \"‚úÖ Configured\" if ELEVENLABS_API_KEY else \"‚ùå Not configured\",\n            \"Audio Processing\": \"‚úÖ Available\",\n            \"Image Processing\": \"‚úÖ Available\",\n            \"Text Processing\": \"‚úÖ Available\"\n        }\n        \n        for service, status in config_status.items():\n            print(f\"{service:20} {status}\")\n        \n        self.demo_results[\"configuration_status\"] = config_status\n        \n        print(\"\\nüìù Setup Notes:\")\n        if not GROQ_API_KEY:\n            print(\"‚Ä¢ Set GROQ_API_KEY in your .env file for image analysis and speech transcription\")\n        if not ELEVENLABS_API_KEY:\n            print(\"‚Ä¢ Set ELEVENLABS_API_KEY in your .env file for premium voice synthesis\")\n        print(\"‚Ä¢ Install required dependencies: pip install -r requirements.txt\")\n        print(\"‚Ä¢ For voice recording, ensure microphone permissions are granted\")\n    \n    def demo_gradio_ui_info(self):\n        \"\"\"Provide information about the Gradio UI.\"\"\"\n        self.print_section(\"GRADIO UI INFORMATION\")\n        \n        print(\"\\nüåê Gradio Multimodal Interface Features:\")\n        print(\"‚Ä¢ Text Chat: Full conversational interface with mental health tools\")\n        print(\"‚Ä¢ Image Analysis: Upload and analyze images for therapeutic insights\")\n        print(\"‚Ä¢ Voice Chat: Record voice messages and get audio responses\")\n        print(\"‚Ä¢ Quick Actions: Emergency help, affirmations, breathing exercises\")\n        print(\"‚Ä¢ Session Management: Multiple conversation sessions\")\n        print(\"‚Ä¢ Configuration Panel: Check API status and settings\")\n        \n        print(\"\\nüöÄ To launch the Gradio interface:\")\n        print(\"python gradio_app.py\")\n        print(\"Then open: http://localhost:7860\")\n        \n        self.demo_results[\"gradio_info\"] = {\n            \"launch_command\": \"python gradio_app.py\",\n            \"default_url\": \"http://localhost:7860\",\n            \"features\": [\"text_chat\", \"image_analysis\", \"voice_chat\", \"quick_actions\"]\n        }\n    \n    def demo_api_endpoints(self):\n        \"\"\"Demonstrate API endpoint information.\"\"\"\n        self.print_section(\"API ENDPOINTS\")\n        \n        endpoints = {\n            \"POST /ask\": \"Text-based conversation\",\n            \"POST /upload-image\": \"Upload image file\",\n            \"POST /upload-audio\": \"Upload audio file\",\n            \"POST /analyze-image\": \"Analyze uploaded image\",\n            \"POST /process-audio\": \"Process audio file\",\n            \"POST /generate-voice\": \"Generate voice from text\",\n            \"POST /multimodal-query\": \"Combined text/image/audio processing\",\n            \"GET /session/{session_id}/history\": \"Get chat history\",\n            \"DELETE /session/{session_id}\": \"Clear session\"\n        }\n        \n        print(\"\\nüîó Available API Endpoints:\")\n        for endpoint, description in endpoints.items():\n            print(f\"{endpoint:35} {description}\")\n        \n        print(\"\\nüöÄ To start the API server:\")\n        print(\"python backend/main.py\")\n        print(\"API docs: http://localhost:8000/docs\")\n        \n        self.demo_results[\"api_endpoints\"] = endpoints\n    \n    async def run_full_demo(self):\n        \"\"\"Run the complete multimodal demonstration.\"\"\"\n        print(\"üåü SAFESPACE AI AGENT - MULTIMODAL DEMO\")\n        print(\"=========================================\")\n        print(\"This demo showcases all multimodal capabilities of the system.\")\n        \n        # Run all demo sections\n        self.demo_configuration_status()\n        await self.demo_text_conversation()\n        self.demo_image_analysis()\n        self.demo_voice_processing()\n        self.demo_audio_info()\n        self.demo_gradio_ui_info()\n        self.demo_api_endpoints()\n        \n        # Summary\n        self.print_section(\"DEMO SUMMARY\")\n        print(\"\\n‚úÖ Demo completed successfully!\")\n        print(f\"üìä Total tests run: {len(self.demo_results)}\")\n        \n        successful_tests = sum(1 for result in self.demo_results.values() \n                             if isinstance(result, dict) and result.get('status') != 'failed')\n        print(f\"‚úÖ Successful operations: {successful_tests}\")\n        \n        print(\"\\nüéØ Next Steps:\")\n        print(\"1. Configure API keys in .env file for full functionality\")\n        print(\"2. Launch Gradio UI: python gradio_app.py\")\n        print(\"3. Start API server: python backend/main.py\")\n        print(\"4. Test multimodal features with real audio and images\")\n        \n        return self.demo_results\n\n\nclass UsageExamples:\n    \"\"\"Practical usage examples for different scenarios.\"\"\"\n    \n    @staticmethod\n    def print_usage_examples():\n        print(\"\\n\" + \"=\"*60)\n        print(\" USAGE EXAMPLES \")\n        print(\"=\"*60)\n        \n        examples = {\n            \"Text Therapy Session\": {\n                \"code\": \"\"\"\n# Simple text conversation\nfrom core.tools import get_general_health_answer\n\nresponse = get_general_health_answer(\"I'm feeling anxious about work\")\nprint(response)\n                \"\"\",\n                \"description\": \"Basic text-based mental health conversation\"\n            },\n            \n            \"Image Analysis\": {\n                \"code\": \"\"\"\n# Analyze an image for emotional insights\nfrom core.tools import analyze_uploaded_image\n\nresponse = analyze_uploaded_image(\n    \"path/to/image.jpg\", \n    \"What emotions do you see in this artwork?\"\n)\nprint(response)\n                \"\"\",\n                \"description\": \"Analyze uploaded images for therapeutic insights\"\n            },\n            \n            \"Voice Processing\": {\n                \"code\": \"\"\"\n# Process voice input\nfrom core.audio_processor import audio_processor\nfrom core.tools import get_general_health_answer\n\n# Record and transcribe\naudio_file, transcription = audio_processor.record_and_transcribe(5)\nprint(f\"You said: {transcription}\")\n\n# Get response and convert to speech\nresponse = get_general_health_answer(transcription)\naudio_response = audio_processor.text_to_speech_gtts(response)\nprint(f\"AI response audio: {audio_response}\")\n                \"\"\",\n                \"description\": \"Complete voice interaction workflow\"\n            },\n            \n            \"Emergency Situation\": {\n                \"code\": \"\"\"\n# Emergency response\nfrom core.tools import emergency_call_tool\n\n# This would trigger emergency protocols\nresult = emergency_call_tool()\nprint(result)\n                \"\"\",\n                \"description\": \"Handle emergency mental health situations\"\n            },\n            \n            \"Gradio Interface\": {\n                \"code\": \"\"\"\n# Launch multimodal web interface\nfrom gradio_app import multimodal_ui\n\n# Start the web interface\nmultimodal_ui.launch(share=False, debug=True)\n# Access at http://localhost:7860\n                \"\"\",\n                \"description\": \"Web-based multimodal interface\"\n            }\n        }\n        \n        for title, example in examples.items():\n            print(f\"\\nüìã {title}:\")\n            print(f\"Description: {example['description']}\")\n            print(\"Code:\")\n            print(example['code'])\n            print(\"-\" * 50)\n\n\nif __name__ == \"__main__\":\n    # Run the comprehensive demo\n    demo = MultimodalDemo()\n    \n    # Run async demo\n    results = asyncio.run(demo.run_full_demo())\n    \n    # Show usage examples\n    UsageExamples.print_usage_examples()\n    \n    print(\"\\nüéâ Demo completed! Check the output above for all features and capabilities.\")